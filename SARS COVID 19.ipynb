# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session



# Importing Libraries

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from tqdm import tqdm
import keras
from keras.utils.np_utils import to_categorical
import cv2
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.layers import  Flatten, Dense, Dropout
from tensorflow import keras
from tensorflow.keras.applications import ResNet50
from keras.layers import Input
from tensorflow.keras.layers import Input, Dense
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.models import Model , Input
from keras import optimizers
from tensorflow.keras.optimizers import Adam
from keras.layers import Dense, GlobalAveragePooling2D
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D
import keras_preprocessing
from keras_preprocessing import image
from keras_preprocessing.image import ImageDataGenerator
from keras import callbacks
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

disease_types = ['COVID', 'non-COVID']
​
train_dir = data_dir = '../input/sarscov2-ctscan-dataset'
​
train_data = []
​
for index, sp in enumerate(disease_types):
    for file in os.listdir(os.path.join(train_dir, sp)):
        train_data.append([sp + "/" + file, index, sp])
        
train = pd.DataFrame(train_data, columns = ['File', 'ID','Disease Type'])
train.head()

Output:: 

File	ID	Disease Type
0	COVID/Covid (230).png	0	COVID
1	COVID/Covid (1195).png	0	COVID
2	COVID/Covid (182).png	0	COVID
3	COVID/Covid (817).png	0	COVID
4	COVID/Covid (631).png	0	COVID

## RANDOMIZING THE ORDER OF TRAINING SET ###

SEED = 42
train = train.sample(frac=1, random_state=SEED) 
train.index = np.arange(len(train)) # Reset indices
train.head()
File	ID	Disease Type
0	COVID/Covid (54).png	0	COVID
1	COVID/Covid (1035).png	0	COVID
2	non-COVID/Non-Covid (21).png	1	non-COVID
3	non-COVID/Non-Covid (248).png	1	non-COVID
4	COVID/Covid (409).png	0	COVID

### SHOWING THE IMAGES OF CT SCAN COVID ###

def plot_defects(defect_types, rows, cols):
    fig, ax = plt.subplots(rows, cols, figsize=(12, 12))
    defect_files = train['File'][train['Disease Type'] == defect_types].values
    n = 0
    for i in range(rows):
        for j in range(cols):
            image_path = os.path.join(data_dir, defect_files[n])
            ax[i, j].set_xticks([])
            ax[i, j].set_yticks([])
            ax[i, j].imshow(cv2.imread(image_path))
            n += 1
# Displays first n images of class from training set
plot_defects('COVID', 5, 5)


### SHOWING THE IMAGES OF CT SCAN NON-COVID ###

# Displays first n images of class from training set
plot_defects('non-COVID', 5, 5)



#### READING THE IMAGE AND RESIZING ###

IMAGE_SIZE = 120
def read_image(filepath):
    return cv2.imread(os.path.join(data_dir, filepath)) 
def resize_image(image, image_size):
    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)
    
    
    
### TRAINING THE IMAGES ###

X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))
for i, file in tqdm(enumerate(train['File'].values)):
    image = read_image(file)
    if image is not None:
        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))
X_Train = X_train / 255.
print(X_Train.shape)

output: 
2481it [00:38, 65.12it/s]
(2481, 120, 120, 3)

### CONVERTING LABLES INTO CATEGORICAL ###

Y_train = train['ID'].values
Y_train = to_categorical(Y_train, num_classes=2)

### TRAIN_TEST_SPLIT ###

BATCH_SIZE = 200

# Split the train and validation sets 
X_train, X_val, Y_train, Y_val = train_test_split(X_Train, Y_train, test_size=0.2, random_state=SEED)

# CHECKING TRAINING IMAGES

fig, ax = plt.subplots(1, 7, figsize=(15, 15))
for i in range(7):
    ax[i].set_axis_off()
    ax[i].imshow(X_train[i])
    ax[i].set_title(disease_types[np.argmax(Y_train[i])])
    




def build_resnet50():
    resnet50 = ResNet50(weights='imagenet', include_top=False)

    input = Input(shape=(SIZE, SIZE, N_ch))
    x = Conv2D(3, (3, 3), padding='same')(input)
    
    x = resnet50(x)
    
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # multi output
    output = Dense(2,activation = 'softmax', name='root')(x)
 
 # model
    model = Model(input,output)
    
    optimizer = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.summary()
    
    return model

EPOCHS =5
SIZE=120
N_ch=3


### DATA AUGMENTATION AND FITTING MODEL ###

model = build_resnet50()
annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.70, patience=5, verbose=1, min_lr=1e-4)
checkpoint = ModelCheckpoint('ResNet50_Model.hdf5', verbose=1, save_best_only=True)
datagen = ImageDataGenerator(rotation_range=360, 
                        width_shift_range=0.2, 
                        height_shift_range=0.2,
                        zoom_range=0.2, 
                        horizontal_flip=True, 
                        vertical_flip=True) 

datagen.fit(X_train)

# Fits the model on batches with real-time data augmentation
hist = model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),
               steps_per_epoch=X_train.shape[0] // BATCH_SIZE,
               epochs=EPOCHS,
               verbose=2,
               callbacks=[annealer, checkpoint],
               validation_data=(X_val, Y_val))
        
Outout:::
Model: "model_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         [(None, 120, 120, 3)]     0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 120, 120, 3)       84        
_________________________________________________________________
resnet50 (Functional)        (None, None, None, 2048)  23587712  
_________________________________________________________________
global_average_pooling2d_2 ( (None, 2048)              0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 2048)              8192      
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0         
_________________________________________________________________
root (Dense)                 (None, 2)                 4098      
=================================================================
Total params: 23,600,086
Trainable params: 23,542,870
Non-trainable params: 57,216
_________________________________________________________________
Epoch 1/5
9/9 - 175s - loss: 1.1556 - accuracy: 0.5611 - val_loss: 0.8162 - val_accuracy: 0.5513

Epoch 00001: val_loss improved from inf to 0.81624, saving model to ResNet50_Model.hdf5
Epoch 2/5
9/9 - 161s - loss: 0.9152 - accuracy: 0.6261 - val_loss: 0.9519 - val_accuracy: 0.5513

Epoch 00002: val_loss did not improve from 0.81624
Epoch 3/5
9/9 - 163s - loss: 0.8313 - accuracy: 0.6541 - val_loss: 1.0035 - val_accuracy: 0.5513

Epoch 00003: val_loss did not improve from 0.81624
Epoch 4/5
9/9 - 166s - loss: 0.7765 - accuracy: 0.7012 - val_loss: 0.8590 - val_accuracy: 0.5513

Epoch 00004: val_loss did not improve from 0.81624
Epoch 5/5


### FINAL LOSS AND ACCURACY ###

#model = load_model('../output/kaggle/working/model.h5')
final_loss, final_accuracy = model.evaluate(X_val, Y_val)
print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))

Output :::
16/16 [==============================] - 15s 897ms/step - loss: 0.8309 - accuracy: 0.5513
Final Loss: 0.8309007287025452, Final Accuracy: 0.5513078570365906'


### accuracy plot ###
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# loss plot
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


### PREDICTION FROM IMAGE  ###


from skimage import io
from keras.preprocessing import image
#path='imbalanced/Scratch/Scratch_400.jpg'
img = image.load_img('../input/sarscov2-ctscan-dataset/COVID/Covid (1010).png', grayscale=False, target_size=(120, 120))
show_img=image.load_img('../input/sarscov2-ctscan-dataset/COVID/Covid (1010).png', grayscale=False, target_size=(200, 200))
disease_class=['Covid-19','Non Covid-19']
x = image.img_to_array(img)
x = np.expand_dims(x, axis = 0)
x /= 255

custom = model.predict(x)
print(custom[0])

plt.imshow(show_img)
plt.show()

a=custom[0]
ind=np.argmax(a)
        
print('Prediction:',disease_class[ind])


